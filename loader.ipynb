{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Loader\n",
    "\n",
    "This notebook is used to load jobs in this repo to the `wandb/jobs` public project.\n",
    "- You will need to be logged into wandb and have access to the `wandb` entity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kill ports ahead of spinning up containers (you may need to restart docker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !lsof -i TCP:3307 | grep LISTEN | awk '{print $2}' | xargs kill -9\n",
    "# !lsof -i TCP:8000 | grep LISTEN | awk '{print $2}' | xargs kill -9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%env WANDB_API_KEY {wandb.api.api_key}\n",
    "%env WANDB_ENTITY wandb\n",
    "%env WANDB_PROJECT jobs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python jobs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The SQL Query job depends on access to a database.  You can load this dummy database with the snippet below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93dcdd9821437d243a2d7354ca2f7375a828ca5b7282c51f73d943af418b0cec\n",
      "docker: Error response from daemon: driver failed programming external connectivity on endpoint happy_mirzakhani (c5bba04dd80bc6614f0560b5eb96766a3369965efeaec80d0cebc2ae5d2e7916): Bind for 0.0.0.0:3307 failed: port is already allocated.\n",
      "env: MYSQL_USER=sakila\n",
      "env: MYSQL_PASSWORD=p_ssW0rd\n"
     ]
    }
   ],
   "source": [
    "!sudo docker run -p 3307:3306 -d sakiladb/mysql:latest\n",
    "%env MYSQL_USER sakila\n",
    "%env MYSQL_PASSWORD p_ssW0rd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run python jobs as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('jobs/sql_query_table/job.py'),\n",
       " PosixPath('jobs/sql_query_artifact/job.py'),\n",
       " PosixPath('jobs/github_actions_workflow_dispatch/job.py'),\n",
       " PosixPath('jobs/msft_teams_webhook/job.py'),\n",
       " PosixPath('jobs/hello_world/job.py'),\n",
       " PosixPath('jobs/http_webhook/job.py')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_jobs = list(Path('jobs').glob('**/*job.py'))\n",
    "python_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NAME=sql_query_table\n",
      "env: WANDB_JOBS_REPO_CONFIG=jobs/sql_query_table/config.yml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmegatruong\u001b[0m (\u001b[33mwandb\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/launch-jobs/wandb/run-20230320_155706-fw511hph\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msql_query_table\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/wandb/jobs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/wandb/jobs/runs/fw511hph\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33msql_query_table\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/wandb/jobs/runs/fw511hph\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 1 media file(s), 4 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230320_155706-fw511hph/logs\u001b[0m\n",
      "env: WANDB_NAME=sql_query_artifact\n",
      "env: WANDB_JOBS_REPO_CONFIG=jobs/sql_query_artifact/config.yml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmegatruong\u001b[0m (\u001b[33mwandb\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/launch-jobs/wandb/run-20230320_155725-283p5s4p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msql_query_artifact\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/wandb/jobs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/wandb/jobs/runs/283p5s4p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33msql_query_artifact\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/wandb/jobs/runs/283p5s4p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230320_155725-283p5s4p/logs\u001b[0m\n",
      "env: WANDB_NAME=github_actions_workflow_dispatch\n",
      "env: WANDB_JOBS_REPO_CONFIG=jobs/github_actions_workflow_dispatch/config.yml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmegatruong\u001b[0m (\u001b[33mwandb\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/launch-jobs/wandb/run-20230320_155742-8ir4uz7m\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgithub_actions_workflow_dispatch\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/wandb/jobs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/wandb/jobs/runs/8ir4uz7m\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mgithub_actions_workflow_dispatch\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/wandb/jobs/runs/8ir4uz7m\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230320_155742-8ir4uz7m/logs\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/launch-jobs/jobs/github_actions_workflow_dispatch/job.py\", line 32, in <module>\n",
      "    Github(token)\n",
      "  File \"/home/ubuntu/miniconda3/envs/py39/lib/python3.9/site-packages/github/NamedUser.py\", line 545, in get_repo\n",
      "    headers, data = self._requester.requestJsonAndCheck(\n",
      "  File \"/home/ubuntu/miniconda3/envs/py39/lib/python3.9/site-packages/github/Requester.py\", line 398, in requestJsonAndCheck\n",
      "    return self.__check(\n",
      "  File \"/home/ubuntu/miniconda3/envs/py39/lib/python3.9/site-packages/github/Requester.py\", line 423, in __check\n",
      "    raise self.__createException(status, responseHeaders, output)\n",
      "github.GithubException.UnknownObjectException: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/reference/repos#get-a-repository\"}\n",
      "env: WANDB_NAME=msft_teams_webhook\n",
      "env: WANDB_JOBS_REPO_CONFIG=jobs/msft_teams_webhook/config.yml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmegatruong\u001b[0m (\u001b[33mwandb\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/launch-jobs/wandb/run-20230320_155758-q7xtidv7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmsft_teams_webhook\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/wandb/jobs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/wandb/jobs/runs/q7xtidv7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mmsft_teams_webhook\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/wandb/jobs/runs/q7xtidv7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 14 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230320_155758-q7xtidv7/logs\u001b[0m\n",
      "env: WANDB_NAME=hello_world\n",
      "env: WANDB_JOBS_REPO_CONFIG=jobs/hello_world/config.yml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmegatruong\u001b[0m (\u001b[33mwandb\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/launch-jobs/wandb/run-20230320_155818-83sp92pq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhello_world\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/wandb/jobs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/wandb/jobs/runs/83sp92pq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: hello world\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mhello_world\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/wandb/jobs/runs/83sp92pq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230320_155818-83sp92pq/logs\u001b[0m\n",
      "env: WANDB_NAME=http_webhook\n",
      "env: WANDB_JOBS_REPO_CONFIG=jobs/http_webhook/config.yml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmegatruong\u001b[0m (\u001b[33mwandb\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/launch-jobs/wandb/run-20230320_155833-bdodhv9h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhttp_webhook\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/wandb/jobs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/wandb/jobs/runs/bdodhv9h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mhttp_webhook\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/wandb/jobs/runs/bdodhv9h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230320_155833-bdodhv9h/logs\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/launch-jobs/jobs/http_webhook/job.py\", line 50, in <module>\n",
      "    r.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda3/envs/py39/lib/python3.9/site-packages/httpx/_models.py\", line 749, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '401 Unauthorized' for url 'https://api.github.com/repos/wandb/launch-jobs/actions/workflows/generate-report.yml/dispatches'\n",
      "For more information check: https://httpstatuses.com/401\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/launch-jobs/jobs/http_webhook/job.py\", line 43, in <module>\n",
      "    for attempt in Retrying(\n",
      "  File \"/home/ubuntu/miniconda3/envs/py39/lib/python3.9/site-packages/tenacity/__init__.py\", line 347, in __iter__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/home/ubuntu/miniconda3/envs/py39/lib/python3.9/site-packages/tenacity/__init__.py\", line 326, in iter\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x7f581b0a86d0 state=finished raised HTTPStatusError>]\n"
     ]
    }
   ],
   "source": [
    "for job in python_jobs:\n",
    "    %env WANDB_NAME {job.parent.name}\n",
    "    %env WANDB_JOBS_REPO_CONFIG {job.parent/'config.yml'}\n",
    "    !pip install -r {job.parent/'requirements.txt'} --quiet\n",
    "    !python {job}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker jobs\n",
    "- These jobs touch AWS, so they mount the `.aws` directory.\n",
    "- If you need to see the literal command, prepend `set -x &&` to the shell command"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker Endpoints job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NAME=deploy_to_sagemaker_endpoints\n",
      "env: WANDB_JOBS_REPO_CONFIG=config_tensorflow.yml\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 301B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9-buster       0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (3/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 301B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.9-buster       0.3s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (11/11) FINISHED                                              \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 301B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.9-buster       0.3s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 200B                                          0.0s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/library/python:3.9-buster@sha256:dda953d28cdcf52  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/6] WORKDIR /launch                                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/6] COPY requirements.txt ./                                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/6] RUN pip install -r requirements.txt                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/6] COPY deploy_to_sagemaker_endpoints.py inference.py ./     0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [6/6] COPY config_pytorch.yml config_tensorflow.yml ./          0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:2ee7188d9eb26f9bdc57058cc29950b824d6e3a8f726c  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/deploy_to_sagemaker_endpoints           0.0s\n",
      "\u001b[0m\u001b[?25hwandb: Currently logged in as: megatruong (wandb). Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.14.0\n",
      "wandb: Run data is saved locally in /launch/wandb/run-20230320_155851-hwvafxsf\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run deploy_to_sagemaker_endpoints\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/wandb/jobs\n",
      "wandb: üöÄ View run at https://wandb.ai/wandb/jobs/runs/hwvafxsf\n",
      "wandb: sagemaker job: Downloading artifact from wandb\n",
      "wandb: Downloading large artifact model-sage-feather-1:v2, 273.69MB. 4 files... \n",
      "wandb:   4 of 4 files downloaded.  \n",
      "Done. 0:0:5.7\n",
      "wandb: sagemaker job: Creating temp directory for sagemaker model\n",
      "wandb: sagemaker job: Uploading model to S3\n",
      "wandb: sagemaker job: Deploy model to Sagemaker Endpoints (this may take a while...)\n",
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "---!The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "wandb: sagemaker job: Successfully deployed endpoint: tensorflow-inference-2023-03-20-15-59-14-510\n",
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: | 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\n",
      "wandb: Run summary:\n",
      "wandb: sagemaker_endpoint tensorflow-inference...\n",
      "wandb: \n",
      "wandb: üöÄ View run deploy_to_sagemaker_endpoints at: https://wandb.ai/wandb/jobs/runs/hwvafxsf\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20230320_155851-hwvafxsf/logs\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_NAME deploy_to_sagemaker_endpoints\n",
    "%env WANDB_JOBS_REPO_CONFIG config_tensorflow.yml\n",
    "\n",
    "!sudo docker build -t $WANDB_NAME jobs/deploy_to_sagemaker_endpoints && \\\n",
    "sudo docker run \\\n",
    "   -v $HOME/.aws:/root/.aws:ro \\\n",
    "   -e WANDB_API_KEY=$WANDB_API_KEY \\\n",
    "   -e WANDB_ENTITY=$WANDB_ENTITY \\\n",
    "   -e WANDB_PROJECT=$WANDB_PROJECT \\\n",
    "   -e WANDB_NAME=$WANDB_NAME \\\n",
    "   -e WANDB_RUN_GROUP=$WANDB_RUN_GROUP \\\n",
    "   -e WANDB_JOBS_REPO_CONFIG=$WANDB_JOBS_REPO_CONFIG \\\n",
    "   $WANDB_NAME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nvidia Triton Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This job requires a running Triton Server.  You can start one with this snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LD_PRELOAD=\"/usr/lib/aarch64-linux-gnu/libgomp.so.1\"\n",
      "ERROR: ld.so: object '\"/usr/lib/aarch64-linux-gnu/libgomp.so.1\"' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/2)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (3/4)                                                         \n",
      "\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for nvcr.io/nvidia/tritonserver:22.11-py3     0.1s\n",
      "\u001b[34m => [auth] nvidia/tritonserver:pull,push token for nvcr.io                 0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (3/4)                                                         \n",
      "\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for nvcr.io/nvidia/tritonserver:22.11-py3     0.3s\n",
      "\u001b[34m => [auth] nvidia/tritonserver:pull,push token for nvcr.io                 0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (3/4)                                                         \n",
      "\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for nvcr.io/nvidia/tritonserver:22.11-py3     0.4s\n",
      "\u001b[34m => [auth] nvidia/tritonserver:pull,push token for nvcr.io                 0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (3/4)                                                         \n",
      "\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for nvcr.io/nvidia/tritonserver:22.11-py3     0.6s\n",
      "\u001b[34m => [auth] nvidia/tritonserver:pull,push token for nvcr.io                 0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.7s (6/6) FINISHED                                                \n",
      "\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for nvcr.io/nvidia/tritonserver:22.11-py3     0.6s\n",
      "\u001b[0m\u001b[34m => [auth] nvidia/tritonserver:pull,push token for nvcr.io                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [1/1] FROM nvcr.io/nvidia/tritonserver:22.11-py3@sha256:1cb912  0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:31b28b6e5433f5bd8e8af980e8bbe7cf4dbc3a75b8325  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/tritonserver-wandb                      0.0s\n",
      "\u001b[0m\u001b[?25hWARNING: Published ports are discarded when using host network mode\n",
      "f420b8e228d36cadf96f9c214f4d77e2e8947b3f79fd7c9254103bad6a981067\n"
     ]
    }
   ],
   "source": [
    "# you may need this export on M1\n",
    "# related: https://github.com/keras-team/keras-tuner/issues/317#issuecomment-640181692\n",
    "%env LD_PRELOAD=\"/usr/lib/aarch64-linux-gnu/libgomp.so.1\"\n",
    "\n",
    "!sudo docker build -t tritonserver-wandb jobs/deploy_to_nvidia_triton/server && \\\n",
    "sudo docker run \\\n",
    "  -v $HOME/.aws:/root/.aws:ro \\\n",
    "  -p 8000:8000 \\\n",
    "  --rm --net=host -d \\\n",
    "  tritonserver-wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then launch the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NAME=deploy_to_nvidia_triton\n",
      "env: WANDB_JOBS_REPO_CONFIG=config_tensorflow.yml\n",
      "ERROR: ld.so: object '\"/usr/lib/aarch64-linux-gnu/libgomp.so.1\"' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 415B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9-buster       0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 415B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9-buster       0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 415B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9-buster       0.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (11/11) FINISHED                                              \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 415B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.9-buster       0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 163B                                          0.0s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/library/python:3.9-buster@sha256:dda953d28cdcf52  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/6] WORKDIR /launch                                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/6] COPY requirements.txt ./                                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/6] RUN pip install -r requirements.txt                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/6] COPY deploy_to_nvidia_triton.py ./                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [6/6] COPY config_pytorch.yml config_tensorflow.yml ./          0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:99252a22ceb99370ef67c18ea383f6e057f746af18655  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/deploy_to_nvidia_triton                 0.0s\n",
      "\u001b[0m\u001b[?25hwandb: Currently logged in as: megatruong (wandb). Use `wandb login --relogin` to force relogin\n",
      "wandb: ERROR wandb version 0.13.8.dev1 has been retired!  Please upgrade.\n",
      "wandb: wandb version 0.14.0 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.13.8.dev1\n",
      "wandb: Run data is saved locally in /launch/wandb/run-20230320_160132-6cvjiyyz\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run deploy_to_nvidia_triton\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/wandb/jobs\n",
      "wandb: üöÄ View run at https://wandb.ai/wandb/jobs/runs/6cvjiyyz\n",
      "wandb: triton job: Downloading wandb artifact\n",
      "wandb: Downloading large artifact model-sage-feather-1:v3, 273.69MB. 4 files... \n",
      "wandb:   4 of 4 files downloaded.  \n",
      "Done. 0:0:5.9\n",
      "wandb: triton job: Uploading model to Triton model repo (this may take a while...)\n",
      "wandb: Uploading keras_metadata.pb to models/model-sage-feather-1/3/model.savedmodel/keras_metadata.pb\n",
      "wandb: Uploading saved_model.pb to models/model-sage-feather-1/3/model.savedmodel/saved_model.pb\n",
      "wandb: Uploading variables/variables.data-00000-of-00001 to models/model-sage-feather-1/3/model.savedmodel/variables/variables.data-00000-of-00001\n",
      "wandb: Uploading variables/variables.index to models/model-sage-feather-1/3/model.savedmodel/variables/variables.index\n",
      "wandb: triton job: Loading model into Triton\n",
      "wandb: WARNING Did not find config.pbtxt for model-sage-feather-1/3.  Trying to autogenerate config...\n",
      "wandb: Using autogenerated config for model-sage-feather-1/3\n",
      "wandb: Generated config at: overloaded_config.pbtxt\n",
      "wandb: triton job: Finished deploying to Triton\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: Synced deploy_to_nvidia_triton: https://wandb.ai/wandb/jobs/runs/6cvjiyyz\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20230320_160132-6cvjiyyz/logs\n",
      "wandb: ERROR wandb version 0.13.8.dev1 has been retired!  Please upgrade.\n",
      "wandb: wandb version 0.14.0 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_NAME deploy_to_nvidia_triton\n",
    "%env WANDB_JOBS_REPO_CONFIG config_tensorflow.yml\n",
    "\n",
    "!sudo docker build -t $WANDB_NAME jobs/deploy_to_nvidia_triton/deployer && \\\n",
    "sudo docker run \\\n",
    "   -v $HOME/.aws:/root/.aws:ro \\\n",
    "   -e WANDB_API_KEY=$WANDB_API_KEY \\\n",
    "   -e WANDB_ENTITY=$WANDB_ENTITY \\\n",
    "   -e WANDB_PROJECT=$WANDB_PROJECT \\\n",
    "   -e WANDB_NAME=$WANDB_NAME \\\n",
    "   -e WANDB_RUN_GROUP=$WANDB_RUN_GROUP \\\n",
    "   -e WANDB_JOBS_REPO_CONFIG=$WANDB_JOBS_REPO_CONFIG \\\n",
    "   --rm --net=host \\\n",
    "   $WANDB_NAME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nvidia Tensor RT Conversion Job\n",
    "This job requires a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NAME=optimize_with_nvidia_tensorrt\n",
      "env: WANDB_JOBS_REPO_CONFIG=config.yml\n",
      "ERROR: ld.so: object '\"/usr/lib/aarch64-linux-gnu/libgomp.so.1\"' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for nvcr.io/nvidia/tensorflow:22.12-tf2-py3   0.1s\n",
      "\u001b[34m => [auth] nvidia/tensorflow:pull,push token for nvcr.io                   0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for nvcr.io/nvidia/tensorflow:22.12-tf2-py3   0.3s\n",
      "\u001b[34m => [auth] nvidia/tensorflow:pull,push token for nvcr.io                   0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for nvcr.io/nvidia/tensorflow:22.12-tf2-py3   0.4s\n",
      "\u001b[34m => [auth] nvidia/tensorflow:pull,push token for nvcr.io                   0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for nvcr.io/nvidia/tensorflow:22.12-tf2-py3   0.6s\n",
      "\u001b[34m => [auth] nvidia/tensorflow:pull,push token for nvcr.io                   0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.7s (9/9) FINISHED                                                \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for nvcr.io/nvidia/tensorflow:22.12-tf2-py3   0.6s\n",
      "\u001b[0m\u001b[34m => [auth] nvidia/tensorflow:pull,push token for nvcr.io                   0.0s\n",
      "\u001b[0m\u001b[34m => [1/3] FROM nvcr.io/nvidia/tensorflow:22.12-tf2-py3@sha256:947e32a2649  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 77B                                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/3] RUN pip install wandb                                     0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/3] COPY optimize_with_tensorrt.py config.yml ./              0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:0d1791b1e19e2faa42c5b2cc1592b82000dd39437e228  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/optimize_with_nvidia_tensorrt           0.0s\n",
      "\u001b[0m\u001b[?25h2023-03-20 16:02:15.893748: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "wandb: Currently logged in as: megatruong (wandb). Use `wandb login --relogin` to force relogin\n",
      "wandb: wandb version 0.14.0 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.13.11\n",
      "wandb: Run data is saved locally in /workspace/wandb/run-20230320_160220-gjsw7wly\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run optimize_with_nvidia_tensorrt\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/wandb/jobs\n",
      "wandb: üöÄ View run at https://wandb.ai/wandb/jobs/runs/gjsw7wly\n",
      "wandb: downloading model\n",
      "wandb: Downloading large artifact inceptionv3:latest, 96.53MB. 4 files... \n",
      "wandb:   4 of 4 files downloaded.  \n",
      "Done. 0:0:2.5\n",
      "2023-03-20 16:02:24.081101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:24.089393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:24.089662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:24.090210: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-20 16:02:24.090799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:24.091055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:24.091336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:24.247077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:24.247405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:24.247695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:24.247970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14790 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "wandb: converting model\n",
      "2023-03-20 16:02:45.836833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:45.837035: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2023-03-20 16:02:45.837225: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-20 16:02:45.837704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:45.837950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:45.838194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:45.838498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:45.838738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:45.838936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14790 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0\n",
      "2023-03-20 16:02:49.328835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:49.329061: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2023-03-20 16:02:49.329233: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-20 16:02:49.329691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:49.329940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:49.330160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:49.330431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:49.330646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:49.330811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14790 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0\n",
      "2023-03-20 16:02:50.485744: W tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc:198] Calibration with FP32 or FP16 is not implemented. Falling back to use_calibration = False.Note that the default value of use_calibration is True.\n",
      "2023-03-20 16:02:50.746089: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:952] \n",
      "\n",
      "################################################################################\n",
      "TensorRT unsupported/non-converted OP Report:\n",
      "\t- NoOp -> 2x\n",
      "\t- Identity -> 1x\n",
      "\t- Placeholder -> 1x\n",
      "--------------------------------------------------------------------------------\n",
      "\t- Total nonconverted OPs: 4\n",
      "\t- Total nonconverted OP Types: 3\n",
      "For more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops.\n",
      "################################################################################\n",
      "\n",
      "2023-03-20 16:02:50.822653: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:1280] The environment variable TF_TRT_MAX_ALLOWED_ENGINES=20 has no effect since there are only 1 TRT Engines with  at least minimum_segment_size=3 nodes.\n",
      "2023-03-20 16:02:50.829484: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:799] Number of TensorRT candidate segments: 1\n",
      "2023-03-20 16:02:51.098654: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:916] Replaced segment 0 consisting of 701 nodes by TRTEngineOp_000_000.\n",
      "wandb: saving converted model\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 94). These functions will not be directly callable after loading.\n",
      "wandb: Adding directory to artifact (./inceptionv3_trt_FP32)... Done. 0.6s\n",
      "wandb: benchmarking models\n",
      "wandb: Warming up...\n",
      "2023-03-20 16:03:02.280206: I tensorflow/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n",
      "wandb: Benchmarking model...\n",
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "wandb: Warming up...\n",
      "wandb: Benchmarking model...\n",
      "wandb: done\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: - 187.131 MB of 187.131 MB uploaded (0.000 MB deduped)\n",
      "wandb: Run history:\n",
      "wandb:  after_opt_inference_time_ms ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñà‚ñÜ‚ñà‚ñá‚ñÖ‚ñÖ\n",
      "wandb: before_opt_inference_time_ms ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñà\n",
      "wandb:            benchmarking_step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:  after_opt_inference_time_ms 34.27005\n",
      "wandb: before_opt_inference_time_ms 146.43788\n",
      "wandb:            benchmarking_step 999\n",
      "wandb: \n",
      "wandb: üöÄ View run optimize_with_nvidia_tensorrt at: https://wandb.ai/wandb/jobs/runs/gjsw7wly\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20230320_160220-gjsw7wly/logs\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_NAME optimize_with_nvidia_tensorrt\n",
    "%env WANDB_JOBS_REPO_CONFIG config.yml\n",
    "\n",
    "!sudo docker build -t $WANDB_NAME jobs/optimize_with_tensor_rt && \\\n",
    "sudo docker run \\\n",
    "    --gpus all \\\n",
    "    --runtime=nvidia \\\n",
    "    -e WANDB_API_KEY=$WANDB_API_KEY \\\n",
    "    -e WANDB_ENTITY=$WANDB_ENTITY \\\n",
    "    -e WANDB_PROJECT=$WANDB_PROJECT \\\n",
    "    -e WANDB_NAME=$WANDB_NAME \\\n",
    "    -e WANDB_RUN_GROUP=$WANDB_RUN_GROUP \\\n",
    "    -e WANDB_JOBS_REPO_CONFIG=$WANDB_JOBS_REPO_CONFIG \\\n",
    "    $WANDB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylaunch39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
